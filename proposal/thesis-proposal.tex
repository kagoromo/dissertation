\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{color}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{float}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{times}

% Editing marks.
\input{../latex_common/editingmarks}

% Other presentational stuff
\input{../latex_common/style}

\begin{document}

\title{Thesis Proposal: \\
  Lattice-based Data Structures for \\
  Deterministic Parallel and Distributed Programming}

\author{Lindsey Kuper \\ Indiana University}

\date{Draft of \today}

\maketitle

\begin{abstract}
  Abstract goes here.
\end{abstract}

\section{Introduction}

\section{Deterministic-by-construction parallel programming}

Parallel programming is notoriously difficult.  \lk{Difficult to who?}
A fundamental reason for this notoriety is that programs can yield
inconsistent results, or even crash, due to unpredictable interactions
between parallel tasks.  But it doesn't have to be this way:
\emph{deterministic-by-construction} parallel programming models offer
the promise of freedom from subtle, hard-to-reproduce nondeterministic
bugs in parallel code.

A deterministic-by-construction programming model is one that ensures
that all programs written using the model have the same
\emph{observable behavior} every time they are run.  What does
``observable behavior'' mean?  There are many ways to define what is
observable about a program.  Clearly, we do \emph{not} wish to
preserve behaviors such as running time across multiple
runs---ideally, a deterministic-by-construction parallel program will
run faster when more parallel resources are available!  In this
proposal, we define the observable behavior of a program to be the
value to which it evaluates.  Specifically, we do not count the
behavior of the scheduler as observable.  Indeed, the purpose of our
deterministic-by-construction model will be to allow tasks to be
scheduled dynamically and unpredictably, without allowing such
\emph{schedule nondeterminism} to affect programs' observable
behavior.

\section{Existing approaches to determinism by construction}

Shared state between computations allows the possibility for
\emph{data races} that allow schedule nondeterminism to be
observed---if one thread writes $3$ to a shared location while another
writes $4$, then another thread that reads and returns the value will
nondeterministically return $3$ or $4$ depending on how the threads
are scheduled to run.  Therefore, deterministic parallel programming
models necessarily limit sharing of mutable state between parallel
tasks.


Introduce the problem.

Explain, at a high level, what my work does to solve the problem.

Explain what the structure of this proposal will be.

This citation \cite{LVars-TR} is here to make LaTeX happy.

\section{Background}

\section{Thesis statement}

\lk{This format ripped off from Josh Dunfield.}

With the above background, I can state my thesis:

\lk{``novel, feasible and useful''}

\lk{ ``Lattice-based data structures, whose contents grow
  monotonically with respect to an application-specific lattice and
  for which the order in which information is added is not observable,
  are a novel, feasible, and useful means of guaranteeing the
  determinism of parallel programs.''  This isn't right.  It's not
  ``here's a parallel program, let's apply lattice-based data
  structures to it and guarantee its determinism that way!''  It's,
  ``Let's build lattice-based data structures into the programming
  model to start.''  }

\begin{quote}
  Lattice-based data structures\lk{, whose contents grow monotonically
  with respect to an application-specific lattice and for which the
  order in which information is added is not observable,} are a
  practical, flexible, and mathematically rigorous foundation for
  deterministic parallel and distributed programming.
\end{quote}

\lk{Better...I'm still unsure about ``and distributed''.}

\section{Technical overview}

\section{Freezing, handlers, and quasideterminism}

\section{LVars and CRDTs}

In this section, I will discuss the relationship between the LVars
model I've described and the mathematical framework of
\emph{conflict-free replicated data types} that distributed systems
research have developed for reasoning about and enforcing the eventual
consistency of distributed replicated objects.

\subsection{Replication and eventual consistency}

Distributed systems often involve \emph{replication} of data objects
across a number of physical locations.  Reasons to replicate data may
include:
\begin{itemize}
\item Robustness to failure: if multiple replicas of an object exist,
  we are less likely to lose that object if part of our system fails.

\item Proximity: objects should be close to clients who need them.

\item Parallelism: different processes simultaneously operating on the
  same object may each need their own copy. \lk{Explain how this is
    different from the above point.}
\end{itemize}
If a system of distributed, replicated objects behaved
indistinguishably from the model a lot of us are used to programming
with, in which all our data and all our computation are on one
machine, then we would not need to concern ourselves with special
techniques to support replication.  \lk{This is worded awkwardly ---
  look at the CRDT papers for inspiration?}  Unfortunately, this is
not the case.

\section{Related work}

\paragraph{Concurrent Revisions}

The Concurrent Revisions (CR)~\cite{concurrent-revisions-haskell11}
programming model uses \emph{isolation types} \cite{isolation-types}
to distinguish regions of the heap shared by multiple mutators.
Rather than enforcing exclusive access in the style of DPJ, CR clones
a copy of the state for each mutator, using a deterministic ``merge
function'' for resolving conflicts in local copies at join points.

Variables can be annotated as being shared between a ``joiner'' thread
and a ``joinee'' thread.  Unlike the least-upper-bound writes of
LVars, CR merge functions are \emph{not} necessarily commutative;
indeed, the default CR merge function is ``joiner wins''.  Determinism
is enforced by the programming model allowing the programmer to
specify which of two writing threads should prevail, regardless of the
order in which those writes arrive.  Hence the states that such a
shared variable take on need not form a lattice.

Still, semilattices turn up in the metatheory of CR: in particular,
Burckhardt and Leijen~\cite{semantics-concurrent-revisions} show that,
for any two vertices in a CR revision diagram, there exists a
\emph{greatest common ancestor} state which can be used to determine
what changes each side has made---an interesting duality with our
model (in which any two LVar states have a lub). \lk{TODO: Write to
  them and ask them if it's a join-semilattice or a meet-semilattice!}

If versioned variables used least upper bound as their merge function
for conflicts\lk{TODO: see if ``versioned variables'' is the term they
  use}, the CR programming model would match that of LVars.  \lk{Think
  of a better way to word that last part.}  But it nevertheless
differs from the LVars model in that, in CR, effects only become
visible at the end of parallel regions. \lk{Check that this is true.}
This precludes the use of traditional lock-free data structures as a
representation for versioned variables.  LVars, on the other hand,
allow asynchronous communication within parallel regions.

\section{Road map}

\bibliographystyle{abbrvnat}
\bibliography{../latex_common/refs}

\end{document}
