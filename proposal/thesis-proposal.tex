\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{color}
\usepackage{hyperref}
\usepackage{stmaryrd}
\usepackage{float}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{times}

% Stuff needed for typesetting LVish and its metatheory
\input{../latex_common/lang}

% Editing marks.
\input{../latex_common/editingmarks}

% Other presentational stuff
\input{../latex_common/style}

%  Use the @ symbol for simple inline code within prose:
\lstMakeShortInline[]@

\begin{document}

\title{Thesis Proposal: \\
  Lattice-based Data Structures for \\
  Deterministic Parallel and Distributed Programming}

\author{Lindsey Kuper \\ Indiana University}

\date{Draft of \today}

\maketitle

\begin{abstract}
  Deterministic-by-construction parallel programming models guarantee
  that programs written using them will have the same observable
  behavior on every run, offering freedom from subtle,
  hard-to-reproduce bugs caused by schedule nondeterminism.  In order
  to guarantee determinism, though, deterministic-by-construction
  models must sharply restrict the sharing of memory between parallel
  tasks: shared memory, if it is allowed at all, is typically limited
  either to single-assignment locations, or to a single type of shared
  data structure, such as a blocking FIFO queue.

  This thesis will show that \emph{lattice-based data structures}, or
  \emph{LVars}, enable an expressive and useful style of
  deterministic-by-construction parallel programming
  that generalizes and unifies previously existing approaches.  LVars
  generalize existing single-assignment models to allow multiple
  assignments that are monotonically increasing with respect to an
  application-specific lattice.  They ensure determinism by allowing
  only monotonic writes and ``threshold'' reads that block until a
  lower bound is reached and do not allow the order of writes to be
  observed.  After presenting the basic LVars model and showing that
  it is deterministic, I will show how to extend it to allow
  non-blocking ``freezing'' reads, resulting in a
  \emph{quasi-deterministic} model in which programs are guaranteed to
  behave deterministically modulo exceptions, and \emph{event
    handlers}, which make it easier to express fixpoint computations
  with LVars.

  Next, I will demonstrate the viability of the LVars model with
  \emph{LVish}, a Haskell library based on it.  The LVish library
  provides a monad in which LVar computations run and a work-stealing
  scheduler that runs user-created threads, together with a library of
  lattice-based data structures.  LVish leverages Haskell's type
  system to index such computations with an \emph{effect level} to
  ensure that only certain LVar effects can occur in a given
  computation, hence statically enforcing determinism or
  quasi-determinism.  I will illustrate the LVish library with running
  examples and present benchmarking results and several case studies
  that demonstrate its applicability.

  Finally, I will investigate the relationship between LVars and
  \emph{conflict-free replicated data types} (CRDTs), which are data
  structures for reasoning about and enforcing the eventual
  consistency of replicated objects in a distributed system.  In one
  direction, I will investigate extending CRDTs to support LVar-style
  threshold reads.  Threshold reads will guarantee that the order in
  which information is added to a CRDT cannot be observed, ensuring a
  greater degree of consistency at the price of read availability.
  In the other direction, I will use techniques from the CRDT
  literature to implement data structures in LVish that support
  non-monotonic updates: PN-Counters, which can be decremented as well
  as incremented, and OR-Sets, from which elements can be removed as
  well as added.
\end{abstract}

\section{Deterministic-by-construction parallel programming}\label{s:intro}

Parallel programming is notoriously difficult.  \lk{Difficult to who?}
A fundamental reason for this difficulty is that programs can yield
inconsistent results, or even crash, due to unpredictable interactions
between parallel tasks.  \emph{Deterministic-by-construction} parallel
programming models, though, offer the promise of freedom from subtle,
hard-to-reproduce nondeterministic bugs in parallel code.

A deterministic-by-construction programming model is one that ensures
that all programs written using the model have the same
\emph{observable behavior} every time they are run.  How do we define
what is observable about a program's behavior?  Clearly, we do
\emph{not} wish to preserve behaviors such as running time across
multiple runs---ideally, a deterministic-by-construction parallel
program will run faster when more parallel resources are available!
In this proposal, then, we define the observable behavior of a program
to be the value to which the program evaluates.  In particular, we do
not count the behavior of the scheduler as observable.  Indeed, the
purpose of our deterministic-by-construction model will be to allow
tasks to be scheduled dynamically and unpredictably, without allowing
such \emph{schedule nondeterminism} to affect programs' observable
behavior.

\subsection{Existing approaches to determinism by construction}

Shared state between computations allows the possibility for
\emph{race conditions} that allow schedule nondeterminism to be
observed.  For instance, if one thread writes $3$ to a shared location
while another writes $4$, then a third thread that reads and returns
the value will nondeterministically return $3$ or $4$ depending on the
order in which the threads are scheduled to run.  Therefore,
deterministic parallel programming models necessarily limit sharing of
mutable state between parallel tasks.

One approach is to allow \emph{no} shared mutable state between tasks,
forcing concurrent tasks to produce values independently.  An example
of no-shared-state parallelism is functional programming with
function-level task parallelism, or \emph{futures}---for instance, in
Haskell programs that use futures by means of the @par@ and @pseq@
combinators~\cite{marlow-par}.  Another approach is pure data
parallelism, as in Data Parallel Haskell~\cite{dph}, and yet another
is forcing references to shared locations to be either immutable or
uniquely owned by a task, as in Deterministic Parallel
Java~\cite{dpj-oopsla}.

Still, some algorithms are more naturally or efficiently written using
shared state or message passing, and hence the development of
deterministic parallel programming models that allow \emph{limited}
sharing of mutable state.  Consider two classic deterministic parallel
programming models, dating back to the late 60s and early 70s:
\begin{itemize}
\item In \emph{Kahn process networks} (KPNs)~\cite{Kahn-1974}, as well
  as in the more restricted \emph{synchronous data flow}
  systems~\cite{lee-sdn}, a network of independent, sequential
  ``computing stations'' communicate with each other through blocking
  FIFO channels.  Each station computes a monotonic function from the
  \emph{history} of its input channels (\ie, the input received so
  far) to the history of its output channels (the output produced so
  far).  KPNs are the basis for deterministic stream-processing
  languages such as StreamIt~\cite{streamit-asplos}.
\item In parallel \emph{single-assignment} languages, ``full/empty''
  bits are associated with memory locations so that they may be
  written to at most once. Single-assignment locations with blocking
  read semantics are known as \emph{IVars}~\emph{IStructures} and are
  a well-established mechanism for enforcing determinism in parallel
  settings: they have appeared in Concurrent ML as
  @SyncVar@s~\cite{reppy-cml-book}; in the Intel Concurrent
  Collections (CnC) system~\cite{CnC}; and have even been implemented
  in hardware in Cray MTA machines~\cite{cray-mta}.  Although most of
  these uses incorporate IVars into already-nondeterministic
  programming environments, the \emph{monad-par} Haskell
  library~\cite{monad-par} uses IVars in a
  deterministic-by-construction setting, allowing user-created threads
  to communicate through IVars without requiring @IO@.  Operations
  that read and write IVars can only run inside a @Par@ monad, thus
  encapsulating them inside otherwise pure programs, and hence a
  program in which the only effects are @Par@ effects is guaranteed to
  be deterministic.
\end{itemize}
In KPNs and other data-flow models, communication takes place over
FIFOs with ever-increasing \emph{channel histories}, while in
IVar-based programming models such as CnC and monad-par, a shared data
store of single-assignment memory locations grows monotonically.
Hence \emph{monotonic data structures}---data structures to which
information is only added and never removed---emerge as a common theme
of both data-flow and single-assignment models.

Because state modifications that only add information and never
destroy it can be structured to commute with each other and thereby
avoid race conditions, it stands to reason that diverse deterministic
parallel programming models would leverage the principle of
monotonicity.  Yet there is little in the way of a theory of monotonic
data structures as a basis for deterministic parallelism.  As a
result, systems like CnC and monad-par emerge independently, without
recognition of their common basis.  Moreover, they lack
\emph{generality}, since in each case, communication is only permitted
through a single type of shared data structure---FIFO queues in KPNs,
for instance, or tables of write-once locations in CnC---limiting the
kinds of algorithms that can be easily expressed.

\subsection{LVars: lattice-based monotonic data structures}

In this thesis, I will show that \emph{lattice-based} data structures,
or \emph{LVars}, enable an expressive and useful style of
deterministic parallel programming that generalizes and unifies
previous existing approaches.  LVars generalize IVars and are so named
because the states an LVar can take on are elements of an
application-specific \emph{lattice}.\footnote{As I will explain in
  Section~\ref{s:lattices}, this ``lattice'' need only be a {\em
    bounded join-semilattice} augmented with a greatest element
  $\top$, in which every two elements have a least upper bound but not
  necessarily a greatest lower bound.  For brevity, I use the term
  ``lattice'' here and in the rest of this proposal.}  This
application-specific lattice determines the semantics of the @put@ and
@get@ operations that comprise the interface to LVars (which I will
explain in detail in Section~\ref{s:lattices}):
\begin{itemize}
\item The @put@ operation can only change an LVar's state in a way
  that is {\em monotonically increasing} with respect to the
  user-specified lattice, because it takes the least upper bound of
  the current state and the new state.

\item The @get@ operation allows only limited observations of the
  state of an LVar.  It requires the user to specify a \emph{threshold
    set} of minimum values that can be read from the LVar, where every
  two elements in the threshold set must have the lattice's greatest
  element $\top$ as their least upper bound.  A call to @get@ blocks
  until the LVar in question reaches a (unique) value in the threshold
  set, then unblocks and returns that value.
\end{itemize}
Together, monotonically increasing writes via @put@ and threshold
reads via @get@ yield a deterministic-by-construction programming
model.  We use LVars to define the \emph{LVish calculus}, a
deterministic parallel calculus with shared state. \lk{I think I
  should call it $\lambda_{LVish}$.}
LVish is expressive enough to subsumes existing deterministic parallel
programming models because it is parameterized by the choice of
lattice.  For example, a lattice of channel histories with a prefix
ordering allows LVars to represent FIFO channels that implement a Kahn
process network, whereas instantiating LVish with a lattice with
``empty'' and ``full'' states (where $\mathit{empty} < \mathit{full}$)
results in a parallel single-assignment language.  Different
instantiations of the lattice result in a family of deterministic
parallel languages.

\lk{Figure out how to include stuff about freezing and handlers in the
  intro.}

\lk{Figure out how to include stuff about CRDTs in the intro.}

\section{Lattices, stores, and determinism}\label{s:lattices}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{figures/example-lvar-lattices.pdf} 
  \caption{Example LVar lattices: (a) positive integers ordered by
    $\leq$; (b) IVar containing a positive integer; (c) pair of
    natural-number-valued IVars, annotated with example threshold sets
    that would correspond to a blocking read of the first or second
    element of the pair.  Any state transition crossing the
    ``tripwire'' for \termfont{getSnd} causes it to unblock and return
    a result.}

  \label{f:lattice-examples}
\end{figure}

\section{Freezing, event handlers, and quasi-determinism}

\section{The LVish library}

\subsection{Case studies}

\section{Thesis statement}

\lk{This format ripped off from Josh Dunfield.}

With the above background, I can state my thesis:

\lk{``novel, feasible and useful''}

\lk{ ``Lattice-based data structures, whose contents grow
  monotonically with respect to an application-specific lattice and
  for which the order in which information is added is not observable,
  are a novel, feasible, and useful means of guaranteeing the
  determinism of parallel programs.''  This isn't right.  It's not
  ``here's a parallel program, let's apply lattice-based data
  structures to it and guarantee its determinism that way!''  It's,
  ``Let's build lattice-based data structures into the programming
  model to start.''  }

\begin{quote}
  Lattice-based data structures\lk{, whose contents grow monotonically
  with respect to an application-specific lattice and for which the
  order in which information is added is not observable,} are a
  practical, flexible, and mathematically rigorous foundation for
  deterministic parallel and distributed programming.
\end{quote}

\lk{Better...I'm still unsure about ``and distributed''.}

\section{LVars and CRDTs}

In this section, I will discuss the relationship between the LVars
model I've described and the mathematical framework of
\emph{conflict-free replicated data types} that distributed systems
research have developed for reasoning about and enforcing the eventual
consistency of distributed replicated objects.

\subsection{Replication and eventual consistency}

Distributed systems often involve \emph{replication} of data objects
across a number of physical locations.  Replication is of fundamental
importance in distributed systems: it makes the system more robust to
data loss and allows for good data locality.  Unfortunately, while it
would be convenient if a system of distributed, replicated objects
behaved indistinguishably from the familiar programming model in which
all data is on one machine and and all computation takes place there,
this is not and cannot be the case.  The famous CAP
theorem~\cite{gilbert-lynch-cap} of distributed computing imposes a
three-way tradeoff between:
\begin{itemize}
\item Consistency, in which every replica sees the same information;
\item Availablility, in which all information is available for both
  reading and writing by all replicas; and
\item Partition tolerance, in which the system is robust to parts of
  it being unable to communicate with one another.
\end{itemize}
An (oversimplified, but useful to a first approximation)
interpretation of the CAP theorem is the slogan, ``Consistency,
availability, and partition tolerance: pick at most two.''  In
practice, real systems must be robust to network partitions and hence
must compromise on at least one of consistency or availability.
Moreover, though, consistency, availability, and partition tolerance
are not binary properties; rather than having, for instance, either
perfect availability or no availability at all, we can choose how much
availability a system must have, then allow less conistency
accordingly.  \emph{Highly available} distributed systems (\eg,
Dynamo~\cite{dynamo}) give up on strict conistency in favor of
\emph{eventual consistency}~\cite{vogels-ec}, in which replicas may
not always agree, but if updates stop arriving, all replicas will
\emph{eventually} come to agree, given enough time.

\subsection{Resolving conflicts between replicas}

How can eventually consistent systems ensure that all replicas of an
object come to agree?  In particular, if replicas differ, how do we
determine which is ``right''?  As a straw man proposal, we could
vacuously satisfy the definition of eventual consistency by setting
all replicas to some pre-determined value---but then, of course, we
would lose all updates we had made to any of the replicas.

As a more practical proposal, we could try to determine which replica
was written most recently, and then have the last writer win.  But
this approach is also less than ideal: even if we had a way of
perfectly synchronizing clocks between replicas and could always
determine which replica was written most recently, just having the
last writer win might not make sense from a \emph{semantic} point of
view.  The developers of Dynamo, Amazon's distributed key-value store,
acknowledged this in their discussion of application-specific
mechanisms for resolving conflicts between replicas~\cite{dynamo}:
\begin{quote}
  The next design choice is who performs the process of conflict
  resolution. This can be done by the data store or the
  application. If conflict resolution is done by the data store, its
  choices are rather limited. In such cases, the data store can only
  use simple policies, such as "last write wins", to resolve
  conflicting updates. On the other hand, since the application is
  aware of the data schema it can decide on the conflict resolution
  method that is best suited for its client’s experience. For
  instance, the application that maintains customer shopping carts can
  choose to "merge" the conflicting versions and return a single
  unified shopping cart.
\end{quote}
In other words, we can take advantage of the fact that, for a
particular application, we know something about the meaning of the
data we are storing, and then parameterize the data store by a
pluggable, application-specific conflict resolution operation.

This notion of application-specific conflict resolution is not without
its problems, especially if implemented in an ad-hoc
way.\footnote{Indeed, as noted in the Dynamo article~\cite{dynamo},
  Amazon's shopping cart presents an anomaly whereby an item removed
  from a cart may re-appear!}

\lk{TODO: Write about CRDTs.}

\section{Related work}

\paragraph{Concurrent Revisions}

The Concurrent Revisions (CR)~\cite{concurrent-revisions-haskell11}
programming model uses \emph{isolation types} \cite{isolation-types}
to distinguish regions of the heap shared by multiple mutators.
Rather than enforcing exclusive access in the style of DPJ, CR clones
a copy of the state for each mutator, using a deterministic ``merge
function'' for resolving conflicts in local copies at join points.

Variables can be annotated as being shared between a ``joiner'' thread
and a ``joinee'' thread.  Unlike the least-upper-bound writes of
LVars, CR merge functions are \emph{not} necessarily commutative;
indeed, the default CR merge function is ``joiner wins''.  Determinism
is enforced by the programming model allowing the programmer to
specify which of two writing threads should prevail, regardless of the
order in which those writes arrive.  Hence the states that such a
shared variable take on need not form a lattice.

Still, semilattices turn up in the metatheory of CR: in particular,
Burckhardt and Leijen~\cite{semantics-concurrent-revisions} show that,
for any two vertices in a CR revision diagram, there exists a
\emph{greatest common ancestor} state which can be used to determine
what changes each side has made---an interesting duality with our
model (in which any two LVar states have a lub). \lk{TODO: Write to
  them and ask them if it's a join-semilattice or a meet-semilattice!}

If versioned variables used least upper bound as their merge function
for conflicts\lk{TODO: see if ``versioned variables'' is the term they
  use}, the CR programming model would match that of LVars.  \lk{Think
  of a better way to word that last part.}  But it nevertheless
differs from the LVars model in that, in CR, effects only become
visible at the end of parallel regions. \lk{Check that this is true.}
This precludes the use of traditional lock-free data structures as a
representation for versioned variables.  LVars, on the other hand,
allow asynchronous communication within parallel regions.

\section{Road map}

I have already completed substantial work towards my thesis:

\begin{itemize}
\item \lk{Some stuff about original LVars paper}
\item \lk{Some stuff about Freeze paper}
\end{itemize}
To complete the thesis, I plan to do the following:
\begin{itemize}
\item \lk{Some stuff about CRDTs and LVars.}
\item \lk{Write the dissertation.}
\end{itemize}
\lk{Time estimates to be added.}

\bibliographystyle{plain}
\bibliography{../latex_common/refs}

\end{document}
