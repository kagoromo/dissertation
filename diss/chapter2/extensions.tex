

\section{Safe, Limited Nondeterminism}\label{section:extensions}

  In practice, a major problem with nondeterministic programs is
  that they can {\em silently} go wrong.
  Most parallel programming models are {\em unsafe} in this sense, 
  but we may classify a nondeterministic language as {\em safe} if all occurrences
  of nondeterminism---that is, execution paths that would yield a wrong
  answer---are trapped and reported as errors.
%% there is a single {\em correct} evaluation for every
%%   program, and any deviation from that evaluation is caught and
%%   ``throws an exception''.  
  This notion of \emph{safe nondeterminism} is analogous to the concept of type
  safety: type-safe programs can throw
  exceptions, but they will not ``go wrong''. 
{%
%% It turns out that there are several desirable features that one
%%   might add to a language that put it in this category of safe
%
%% Various useful extensions to a deterministic language place it in this
%%  class of safe nondeterministic languages.
%
We find that there are various extensions
to a deterministic
language that make it safely nondeterministic.\footnote{For instance, while not recognized explicitly by the
   authors as such, a recent extension to CnC for memory management \cite{cnc-space-folding} incidentally fell into this
   category.}  
Here, we will look at one such extension:
{\em exact but destructive observations}.}

%
We begin by noting
that when the state of an LVar has come to rest---when no
more $\PUT$s will occur---then its final value is a deterministic
function of program inputs, and is therefore safe to read directly, 
rather than through a thresholded $\GET$.
For instance, once no more elements will be added to
the @l_acc@ accumulator variable in the @bf_traverse@
example of Figure~\ref{f:bfs-lvar},
  it is safe to read the exact, complete set contents.

The problem is determining automatically {\em when} an LVar has come to rest;
usually, the programmer must determine this based on the control flow of the
program.  We may, however, provide a mechanism for the programmer to place their bet.
{\em If} the value
of an LVar is indeed at rest, then we do no harm to it by
corrupting its state in such a way that further modification will lead to an
error.  We can accomplish this by adding an extra state, called
$\textit{probation}$, to $D$.  The lattice defined by the relation
$\myleq$ is extended thus:
%
\begin{align*}
\textit{probation} &\myleq \top \\
\forall d \in D.~d &\notmyleq \textit{probation}
\end{align*}
\noindent
We then propose a new operation, $\CONSUME$, that takes a pointer to an LVar $l$, updates the store, setting $l$'s state to $\textit{probation}$, and returns a singleton
  set containing the {\em exact} previous state of $l$, rather than a lower bound on that state.
The idea is to ensure that, after a $\CONSUME$, any further operations 
on $l$ will go awry:
% $\GET$ operations will block indefinitely, and 
$\PUT$ operations will
attempt to move the state of $l$ to $\top$, 
resulting in $\error$.

% of Figure~\ref{f:lattice-examples}(c), 

%% {The basic $\lambdapar$ model, as we've presented it, is useful for
%%   unifying a body of work on existing programming models as well as suggesting some
%%   generalizations to real libraries like {\tt monad-par}.  However, we
%%   believe that $\lambdapar$ is also useful as a starting point for
%%   further extension and exploration, including limited forms of
%%   nondeterminism.  }

%% In this section we give an example of a natural extension of
%% $\lambdapar$ that provides additional capabilities.  We use as our
%% motivating example {\em asynchronous parallel reductions}
%% \cite{newton-asynchronous-reduction}, which are reductions (also known as \emph{fold}s) that are
%% disassociated from the parallel control flow of a program.
%% Contributions to the reduction can come from any ``thread'' at any
%% time, as can a demand for the final result.

%% \rn{THE FOLLOWING PARA CAN BE AXED FOR SPACE:}

%% \lk{I would actually prefer to keep this para if at all possible!
%%   It's really good to have the concrete TBB example.}

%% \textred{
%% By contrast, many parallel programming mechanisms provide {\em
%%   synchronous} reduction mechanisms, where the reduction is bound to
%% the explicit (typically fork-join) parallel control flow.
%% %
%% Often it is even known exactly how many parallel computations will
%% participate in the reduction before the parallel region is entered
%% (as in the Intel Threading Building Blocks library's {\tt
%%   parallel\_reduce} \cite{tbb}).
%% %
%% Typically the programmer may only: (1) initiate a
%% parallel loop, (2) associate a reduction with that loop, (3) retrieve
%% the value of the reduction only in the sequential region after the
%% loop completes, \ie, after the barrier.  This mechanism is less flexible than asynchronous reductions.
%% }



In the following example program, we use $\CONSUME$ to perform an
asynchronous sum reduction over a known
number of inputs.  {In such a reduction, data dependencies
alone determine when the reduction is complete, rather than control constructs
such as parallel loops and barriers.}
% , but in an otherwise asynchronous environment.

\begin{equation}\label{e:consume}
\begin{split}
& \letexp{\mathit{cnt}}{\NEW}{ \\
& \letspace \letexp{\mathit{sum}}{\NEW}{ \\
& \letspace \letspace \LETPAR      ~p_1 = (\BUMP_3~{\mathit{sum}};\; \PUT ~ \{a\} ~ \mathit{cnt})\\
& \letspace \letspace \letparspace ~p_2 = (\BUMP_4~{\mathit{sum}};\; \PUT ~ \{b\} ~ \mathit{cnt})\\
& \letspace \letspace \letparspace ~p_3 = (\BUMP_5~{\mathit{sum}};\; \PUT ~ \{c\} ~ \mathit{cnt})\\
& \letspace \letspace \letparspace ~r  \hspace{0.47em} = (\getexp{\mathit{cnt}}\{a,b,c\};\; \consumeexp{\mathit{sum}})\\
& \letspace \letspace \letspace \IN~ \dots~r~\dots
}}
\end{split}
\end{equation}
In \eqref{e:consume},
we use semicolon for sequential composition: $e_1; e_2$ is sugar for \letexp{\mathit{\_}}{e_1}{e_2}.
We also assume a new syntactic sugar in
the form of a $\BUMP$ operation that takes a pointer to an LVar representing a counter and increments it by one,
with $\BUMP_n~l$ as an additional shorthand for $n$ consecutive
$\BUMP$s to $l$.
%% LK: this footnote is redundant now.
%% \footnote{Strictly 
%% speaking, if we directly use the lattice of Figure~\ref{f:lattice-examples}(c), the $\BUMP$ operation would not
%%   be possible.  However, we can simulate such a lattice using a power-set
%%   lattice ordered by subset inclusion.  We give a desugaring of
%%   programs using $\BUMP$ in our technical report \cite{lampar-TR}.}
% The $\BUMP$ operation is used with infinite lattices, 
Meanwhile, the $\mathit{cnt}$ LVar uses the power-set lattice of the set $\{a,b,c\}$ to track the completion
of the $p_1$, $p_2$ and $p_3$ ``threads''.\lk{If we had room, we could point out that the symbols $a$, $b$, and $c$ are not important; any power-set lattice of a set of three elements would work here.}


%% The program is written using two different variables (a count and a
%% sum), but it could just as well be written with a single variable
%% storing a pair of counters.
Before the call to $\CONSUME$, $\getexp{cnt}\{a,b,c\}$ serves
as a synchronization mechanism, ensuring that all increments are
complete before the value is read.
Three writers and one reader execute in parallel, and only
when all writers complete does the reader return the sum,
which in this case will be $3 + 4 + 5 = 12$.
% $\lbrace 12 \rbrace$ .
%
%% One may comment that the program could be simpler if we moved
%% the $\CONSUME$ after the final $\IN$, using the implicit
%% barrier to synchronize the reduction.  This is true; nevertheless, the desired
%% example is one of asynchronous reduction where data dependencies alone
%% suffice \cite{newton-asynchronous-reduction}.  

% TODO: rework SAFETY here.... type safety / memory safety

The good news is that \eqref{e:consume} is deterministic; it
will always return the same value in any execution.  However,
the  $\CONSUME$ primitive in general admits safe nondeterminism,
meaning that, while all runs of the program will terminate with the
same value {\em if} they terminate without error, some runs of the
program may terminate in $\error$, in spite of other runs completing
successfully.
%% \begin{itemize}
%% \item {\bf (Property 1)} All runs of the program will terminate with the same value {\em
%%     if} they terminate without error.
%% \item {\bf (Property 2)} Some runs of the program may terminate in $\error$, in spite
%%   of other runs completing successfully.
%% \end{itemize}
To see how an error might occur, imagine an alternate version of \eqref{e:consume} 
in which $\getexp{cnt}{\{a,b,c\}}$ is replaced
by $\getexp{cnt}{\{a,b\}}$.  This version would have
insufficient synchronization.  The program could run correctly many
times---if the $\BUMP$s happen to complete before the
$\CONSUME$ operation executes---and yet step to $\error$ on the
thousandth run.
%
{Yet, with safe nondeterminism, it is possible to catch and respond to
this error, for example by rerunning in a debug mode that is guaranteed
to find a valid execution if it exists, or by using a {\em data-race detector} which will
reproduce all races in the execution in question.
%
\new{In fact, the simplest form of error handling is to simply retry (or rerun
  from the last snapshot)---most data races make it into production only because 
  they occur {\em rarely}.}
%% Further, this new class of languages opens up the possibility of
%%   simply retrying programs that fail, perhaps ....
%%   in the case of long running programs.
We have implemented a proof-of-concept interpreter and data-race detector for
$\lambdapar$ extended with $\CONSUME$, available in the LVars repository}.

%
%% The algorithm uses the notion of pedigree in
%% Figure~\ref{f:desugar}, except that $\GET$ and $\PUT$ are rewritten to
%% include an additional $p$ argument for pedigrees, \ie,
%%   $\lambda p .~ \getexp{(\app{\trans{a}}{\consL{p}})}
%%                        {(\app{\trans{b}}{\consR{p}})} ~p$.
%% Next, the program is run with a modified implementation of
%% $\GET$/$\PUT$/$\CONSUME$ that logs all effects to each location, along
%% with their pedigree.\footnote
%% {Unfortunately, while there are significant results reducing the space
%%   cost of data-race detection for strictly nested, fork-join parallel models
%%   \cite{race-detector-data-structures}, for the more general class of dependence
%%   graphs induced by $\lambdapar$, there are not yet known methods for reducing the space
%%   requirements for data-race detection to less than the space required
%%   to store the entire dynamic dependence graph itself.}
%% %
%% % In future work we will explore reducing the space requirements of this algorithm.
%% %
%% The $LRJ$ pedigrees of Figure~\ref{f:desugar} already create a partial
%% order, where, for example:
%% % \noindent
%% \begin{itemize}
%% \item $p < \consR{p}$,\; \ie{} $p$ is {\em earlier} in time than its right child,
%% \item $\forall q ~.~ {q}{:}\consR{p} < \consJ{p}$,\; 
%%    all pedigrees within a branch happen earlier than the join point.
%% \end{itemize}
%% Pedigrees {\em not} ordered by this partial order occur {\em in parallel}.
%% Communication through LVars further constrains this time ordering (much
%% like \emph{vector clocks}).  The log of effects is aggregated
%% into a table of additional ordering constraints: \ie, a $\PUT$ that
%% unblocks a parallel $\GET$ adds an edge.  In this enhanced
%% relation, if any $\CONSUME$ operation is not strictly later in time than
%% a $\PUT$ or $\GET$ on the same location, then that $\CONSUME$ is
%% insufficiently synchronized.


% \subsubsection*{Example 3B: Pair with a Counter}
% \textred{
% What would, say, a pair with an integer in the car and a counter in
% its cdr look like?  Well here's where we need that "bump" edge to be
% distinguished from others.
% }

% \begin{verbatim}
%    tierinf: top ....  
%    tier3: (bot,3) (1,2) (2,2) ...  
%    tier2: (bot,2) (1,1) (2,1) ...  
%    tier1: (bot,1) (1,0) (2,0) ...  
%    tier0: bot
% \end{verbatim}
% \textred{ "bot" is equivalent to (bot,0) }
% \textred{
% In the above textual representation the "bump" edges take us straight
% upward.  Bumping can happen before and after setting the car.  There
% *are* multiple upward edges from the bot state, yet only one of them
% is labeled "bump" -- the edge from bot -> (bot,1).
% }


% \subsection{Syntactic Sugar for Counting}
\subsection{Desugaring $\BUMP$}\label{subsection:bump}

{%% \eqref{e:consume} uses atomic counters, which require a $\BUMP$
 %%  operation.
In this section we explain how the $\BUMP$ operation for LVar counters---as well as an underlying
  capability for generating unique IDs---can be implemented in $\lambdapar$.}

Strictly speaking, if we directly used the atomic counter lattice of Figure~\ref{f:lattice-examples}(c) for the $\mathit{sum}$ LVar in \eqref{e:consume}, we would not
  be able to implement $\BUMP$.
%% We will use a sum reduction, in which we fold the $+$ operator over natural numbers, as our
%% example.  To do so, we will need to represent
%% shared, increment-only counters as a $\lambdapar$ domain
%% like that shown in Figure~\ref{f:lattice-examples}(c).
%% \rn{Note this domain does NOT look like fig 1(c); it's a powerset lattice that ultimately simulates 1(C)}
%% %
%% Doing so does not require an extension to $\lambdapar$, but it merits
%% additional syntactic sugar in our meta-notation.
%% \lk{I honestly find this part of the paper pretty opaque.  I'm not
%%   sure how to explain it more clearly without taking up a lot more
%%   space, though!  :( }
Rather than use that lattice directly, then, we can simulate it 
using a power-set lattice over an arbitrary alphabet of symbols
$\lbrace s_1, s_2, s_3, \ldots\rbrace$, ordered by subset inclusion.
%
LVars whose states occupy such a lattice encode natural numbers using the cardinality
of the subset.\footnote{Of course, just as with an encoding like
  Church numerals, this encoding would never be used by a realistic
  implementation.}  
% LK: This was actually wrong.
%% Thus, a blocking $\GET$ operation
%% that unblocks when the count reaches, say, $3$ would
%% take a threshold set enumerating all the three-element subsets
%% of the alphabet.
With this encoding, incrementing a shared variable $l$
requires $\putexp{l}{\stateset{\alpha}}$, where 
  $\alpha \in \lbrace s_1, s_2, s_3, \ldots\rbrace$ and 
$\alpha$ has not previously been used.
% ; i.e. it is a fresh ``gensym''.
%  is a {\em unique symbol in  the alphabet}.  
Rather than requiring the programmer to be responsible for creating a
unique $\alpha$ for each parallel contribution to the counter, though,
we would like to be able to provide a language construct @unique@
that, when evaluated, returns a singleton set containing a single
unique element of the alphabet: $\stateset{\alpha}$. The expression $\BUMP~l$ could then simply desugar to $\putexp{l}{\impfnt{unique}}$.

Fortunately, @unique@ is implementable: well-known techniques exist for generating a unique (but
schedule-invariant and deterministic) identifier for a given point in
a parallel execution.
One such technique is to reify the position of an
operation inside a tree (or DAG) of parallel evaluations.  
% By analogy,
% This is something like an {instruction counter} for a serial abstract
% machine, generalized to a parallel setting.
The Cilk Plus parallel programming language 
% \cite{cilk-plus} 
refers to this notion as the {\em pedigree} of an operation and uses it to seed a
deterministic parallel random number generator \cite{cilk-dprng}.

Figure~\ref{f:desugar} gives one possible set of rewrite rules for a transformation that uses the pedigree technique to desugar $\lambdapar$ programs containing @unique@.  Bearing some resemblance to a continuation-passing-style transformation \cite{cps-citation}, it 
creates a tree that tracks the dynamic evaluation of
applications.
We have implemented this transformation as a part of our 
interpreter for $\lambdapar$ extended with $\CONSUME$.
With \impfnt{unique} in place, we can write programs like the following, in which two
parallel
computations increment the same counter:
\begin{equation*}
\begin{split}
& \letexp{\mathit{sum}}{\NEW}{ \\
& \letspace \LETPAR      ~p_1 = (\putexp{\mathit{sum}}{\impfnt{unique}};\; \putexp{\mathit{sum}}{\impfnt{unique}})\\
& \letspace \letparspace ~p_2 = (\putexp{\mathit{sum}}{\impfnt{unique}})\\
& \letspace \letspace \letspace \IN~ ...
}
\end{split}
\end{equation*}
In this case, the $p_1$ and $p_2$ ``threads'' will together
increment the sum by three.  Notice that consecutive increments
performed by $p_1$ are not atomic.  
% We can add by increments greater than one either by series of puts like above, or by changi

\begin{figure}
\footnotesize
    \begin{align*}
      \trans{\UNIQUE}         &= \lambda p .~ \reflectexp{p} \\
      \trans{v}               &= \lambda p .~ v \\
      \trans{Q}              &= \lambda p .~ Q \\     
      \trans{\lam{v}{e}}     &= \lambda p .~ \lambda v .~ \trans{e} \\
      \trans{\NEW}           &= \lambda p .~ \NEW \\
      \trans{\app{e_1}{e_2}}   &= \lambda p .~ (\app{\app{(\trans{e_1} ~ \consL{p})}{(\trans{e_2} ~ \consR{p})}} \consJ{p}) \\
      \trans{\putexp{e_1}{e_2}}  &= \lambda p .~ \putexp{(\app{\trans{e_1}}{\consL{p}})} 
                                                    {(\app{\trans{e_2}}{\consR{p}})} \\
      \trans{\getexp{e_1}{e_2}}  &= \lambda p .~ \getexp{(\app{\trans{e_1}}{\consL{p}})}
                                                    {(\app{\trans{e_2}}{\consR{p}})} \\
      \trans{\reifyexp{e}}    &= \lambda p .~ \reifyexp{(\app{\trans{e}}{p})} 
     %% \trans{\reifyexp{e}}     & = \lambda p .~ \REIFY(\app{\trans{e}}{p}) \\
     %% \trans{\reflectexp{e}}   & = \lambda p .~ \REFLECT(\app{\trans{e}}{p}) \\
    \end{align*}
  \caption{\footnotesize Rewrite rules for desugaring $\lambdapar$ expressions with the \lstinline|unique| construct to plain $\lambdapar$ expressions without \lstinline|unique|.  Here we use ``$L$:'', ``$R$:'', and ``$J$:'' to
    {\em cons} onto the front of a list that represents a path within
    a fork/join DAG.  These prefixes mean, respectively, ``left branch'',
    ``right branch'', or ``after the join'' of the two branches.
%
    This transformation requires a $\lambda$-calculus encoding of lists, as well as a definition
    of $\REFLECT$ that is an injective function from these list values
    onto the alphabet of unique symbols.
%
    %% An implementation and unit tests for the above transformation can
    %% be found in the \myhref{\interpRepo}{github repository}.
 %   {\url{http://github.com/rrnewton/lambdapar_interps}}.
}
  \label{f:desugar}
\end{figure}
%% The desugaring algorithm is shown in Figure~\ref{f:desugar}.  

%% The $\UNIQUE$ desugaring of Figure~\ref{f:desugar} also requires an
%% encoding of lists for ``{\tt L:R:L..}'' path terms, and a definition
%% of $\REFLECT$ that is an injective function from these list values
%% onto the alphabet $\lbrace a, b, c, \ldots\rbrace$.



%% In the context of
%% $\lambdapar$, there is an interplay between $\lambda$-calculus
%% expressions and elements $d$ of the domain $D$, 
%%   with conversions performed by $\REIFY$ in one direction and by 
%% its inverse, the $\REFLECT$ function mentioned in
%% Section~\ref{subsection:kpns}, in the other.
%% In this case, Figure~\ref{f:desugar} assumes that it is possible to maintain a
%% list of Boolean ({\tt L}/{\tt R}) values and $\REFLECT$ those lists into 
%% domain elements when needed.
%% \lk{Not sure why we need this sentence:  
%% ``Recall that the return values of $\UNIQUE$ must only be
%% arbitrary elements of a set about which we assume nothing.''}


%% Of course, lists can be encoded in the $\lambda$-caluculus (similarly
%% to integers \cite{list-encoding-in-lambda-calc})

