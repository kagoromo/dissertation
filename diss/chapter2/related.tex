\section{Related Work}
\label{section:related}

\if{0}
Work on deterministic parallel programming
models is long-standing; some milestones include:

\begin{itemize}
\item 1968: {\em single-assignment} parallel languages \cite{Tesler-1968};
\item 1974: Kahn process networks \cite{kahn-1974};
\item 1980s: data-flow computing \cite{IStructures}, data-parallel
              languages \cite{Sabot_Paralation};
\item 1990s: continued progress on parallel functional programming 
      \cite{ph,blelloch_something,hammond_par_90s};
\item 2000s: introduction of type systems powerful enough to track
  and restrict effects in imperative programs \cite{par_by_default};
  new data-parallel models such as MapReduce \cite{MapReduce}.
\end{itemize}
\fi{}

Work on deterministic parallel programming
models is long-standing.
In addition to the single-assignment and KPN models already
discussed, here we consider a few recent contributions to the literature.

\paragraph{Deterministic Parallel Java (DPJ)}

% As we have presented it, $\lambdapar$ enforces single assignment
% dynamically, but it is also compatible with static techniques, such as
% affine type systems, to enforce that LVar locations are handled
% linearly.
% \lk{I don't think we should say this!  Affine types, linear types,
%   etc., would be fine for enforcing single assignment \emph{if} that's
%   all we had to do, but I'm pretty sure they can't enforce monotonic
%   multiple assignment; you would need something fancier for that.}
%   Moreover, in the presence of linearity guarantees it is
% possible to remove the restrictions on $\GET$ and $\PUT$.  The proof
% of determinism would rest on the fact that no other reduction would be
% concurrently possible on a linear location.\rn{Lindsey -- what do you
%   think of that?  Fair statement?  Tweak?}
% \lk{Wait.  An affine type system is enough to enforce single
%   assignment, but how would it be enough to enforce monotonic multiple
%   assignment?  Moreover, even if the affine type system can enforce
%   single assignment in a particular thread, how would it prevent
%   different threads from writing to the same location?  I don't know
%   how you would enforce that prior to runtime except by serializing
%   the whole program.}

DPJ \cite{dpj-hotpar09} is a deterministic language
consisting of a system of annotations for Java code.  A sophisticated
region-based type system ensures that a mutable region of the heap is,
essentially, passed linearly to an exclusive writer.
While a linear type system or region system like that of DPJ
could be used to enforce single assignment statically, accommodating $\lambdapar$'s
semantics would involve parameterizing the
type system by the user-specified lattice.
%% ---a direction of
%% inquiry that we leave for future work.

DPJ also
provides a way to unsafely assert that operations commute with one another (using the @commuteswith@ form) to enable
concurrent mutation.  However, DPJ does not provide direct support for
modeling message-passing (\eg, KPNs) or asynchronous communication
within parallel regions.  
\rn{DOUBLE CHECK THIS.}
Finally, a key difference between the $\lambdapar$ model and DPJ 
is that $\lambdapar$ retains determinism by restricting {\em
  what} can be read or written, rather than by restricting 
%% {\em who} can read or write.
the semantics of reads and writes themselves.


%% We believe that DPJ's techniques are largely orthogonal to, and
%% compatible with, the work we have presented on $\lambdapar$.
%
%% \new{For example, while $\lambdapar$, like most systems based on
%%   $IVars$, relies on dynamic enforcement of $\PUT$ errors, it would
%%   also be possible to use DPJ-style region annotations to remove the
%%   need for dynamic checks.}

% \rn{One distinction vs. DPJ and other strictly nested or BSP models
%   is that lambda-par models programs in the asynchronous/dataflow
%   tradition, which is getting a LOT of traction recently... Akka
%   dataflow concurrency, Chapel, etc etc, making it important to have
%   semantic foundations for this class of programming models.}

% \rn{I think something we need to argue is that we are more EXPRESSIVE
%   than DPJ.  DPJ is in some sense not as well motivated.  They want to
%   make certain restricted imperative programs deterministic (mostly
%   divide-and-conquer it seems), but what's the strong motivation when
%   you can just write those in a purely functional way?  We, on the
%   other hand, are talking about something DOES NOT come for free with
%   using Haskell.. communicating, asynchronous agents.}

% \note{Essentially the OPPOSITE of $\lambdapar$, focused primarily on
%   inplace operations on disjoint memory, no communication and
%   synchronization within parallel regions.  We focus on which building
%   up a class of safe operations on shared state.}


\paragraph{Concurrent Revisions}
The Concurrent Revisions (CR) 
% \cite{concurrent-revisions-oopsla} 
\cite{concurrent-revisions-haskell11}
%http://research.microsoft.com/pubs/151805/PrettierConcurrency-Haskell2011.pdf
programming model uses isolation types to distinguish regions of the heap shared
by multiple mutators.  Rather than enforcing exclusive access, CR
clones a copy of the state for each mutator, using a
deterministic policy for resolving conflicts in local copies.
% (hence the name of the system, which draws inspiration from version control systems).  
%
The management of shared variables in CR is tightly coupled to a
fork-join control structure, and the implementation of these variables
is similar to reduction variables in other languages (\eg, Cilk
{\em hyperobjects}).
% \cite{cilk-hyperobjects}).
%
CR charts an important new area in the deterministic-parallelism
design space, but one that differs significantly from
% explores a very different part of the design space that
$\lambdapar$.  CR could be used to model similar types of data
structures---if versioned variables used least upper bound as their merge function
for conflicts---but effects would only become visible at the end of
parallel regions, rather than $\lambdapar$'s asynchronous communication within
parallel regions.

% , targeting imperative multiple-assignment and {\em not}
% asynchronous, data-driven communication.

% \note{Nevertheless, CR could be used to build a practical
%   implementation of a $\lambdapar$-like programming model by defining
%   LVars as shared CR versioned variables the merge operation as lub.}

\paragraph{Bloom and Bloom$^L$}
\lk{Augh, I really want to mention CRDTs, but no room!}
% Turning our attention to research from the distributed systems community,
% Bloom \cite{bloom-website} is a declarative, domain-specific language 
% embedded in Ruby, designed for expressing order-independent, distributed
% computations that reach \emph{eventual consistency} \cite{eventual-consistency}.

%% In the database literature, there is a body of work on {\em monotonic
%%   logic} that has recently been leveraged by the {Bloom language} 
%% to achieve \emph{eventual consistency} in distributed programs.  
% Bloom \cite{bloom-website} is a language, embedded in Ruby, for programming such systems.
%
In the distributed systems literature, \emph{eventually
  consistent} systems \cite{eventual-consistency} 
leverage the idea of monotonicity to guarantee that,
for instance, nodes in a distributed database eventually agree.
The Bloom language for distributed database programming \cite{bloom-cidr} guarantees eventual consistency for
distributed data collections that are updated monotonically.
The initial formulation of Bloom 
had a notion of monotonicity based on \emph{set containment},
analogous to the store ordering for single-assignment languages given in 
Definition~\ref{def:leqstore-cnc}.
However, recent work
by Conway \etal~\cite{blooml} generalizes Bloom to a more flexible
lattice-parameterized system, Bloom$^L$, in a manner analogous
to our generalization from IVars to LVars.
Bloom$^L$ comes with a library of built-in lattice types and also
allows for users to implement their own lattice types as Ruby classes.
Although Conway \etal~do not give a proof of eventual consistency for
Bloom$^L$, our determinism result for $\lambdapar$ suggests that
%% generalizing from a set-containment-based 
%% notion of monotonicity to
%% one parameterized by a user-defined lattice 
their generalization is indeed safe.
Moreover, although the goals of Bloom$^L$
% (being a pure language with no shared state)
differ from those of LVars, we believe that Bloom$^L$ bodes well for
programmers' willingness to use lattice-based data structures,
and lattice-parameterized languages based on them, to address
real-world programming challenges.

\if{0}
Since the goal of Bloom$^L$ is to enforce eventual consistency among
distributed replicas (rather than determinism), a read of a variable
in Bloom$^L$ is essentially a write into a local replica of the least
upper bound of all other replicas' copies of its value.  Therefore
Bloom$^L$ does not use threshold reads, which are necessary in the
shared-state LVar setting.
\fi{}

%% LK: Axed for space. :(
%% \paragraph{Quantum programming}
%% The $\lambdapar$ semantics is reminiscent of the semantics of quantum
%% programming languages
%% that extend a conventional $\lambda$-calculus with a store
%% that maintains the quantum state. Because of quantum parallelism, the
%% quantum state can be accessed by many threads in parallel, but
%% only through a restricted interface.
%% As a concrete example, the language
%% designed by Selinger and Valiron~\cite{valiron} allows only the following
%%  operations on quantum data:
%% (1) ``appending'' to the current data using the tensor product;
%% (2) performing a unitary operation that must, by definition, act linearly and
%% uniformly on the data; and
%% (3) selecting a set of orthogonal subspaces and performing a measurement
%% that projects the quantum state onto one of the subspaces.
%% These operations correspond roughly to $\lambdapar$'s $\NEW$, $\PUT$, and $\GET$.  
%% Quantum mechanics may serve as a source of inspiration
%% when designing operations like $\CONSUME$ that introduce limited
%% nondeterminism.


