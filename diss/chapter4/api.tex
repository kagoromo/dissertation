\section{The LVish library API}\label{section:lvish-api}

\TODO{Edit this section.  It needs to cover effect levels, not just
  determinism levels.  Also, add examples from the lvar-examples
  repo.}

\lk{Following is dumped in from the FHPC paper.}

We have implemented a prototype LVars library based on
the {\em monad-par} Haskell library, which provides the @Par@ monad \cite{monad-par}.
Our library, together with example programs and preliminary
benchmarking results, is available in in the LVars repository.
{The relationship to $\lambdaLVar$ is somewhat loose: for instance,
  while evaluation in $\lambdaLVar$ is always strict, our library allows
  lazy, pure Haskell computations along with strict, parallel monadic computations.}

%\footnote{Not literally, but as System F underpins Haskell.}.

%% If we could use arbitrary monotonic data structures, then multiple threads could
%% traverse the connected component, adding node-labels to a {\em set}, with the
%% function $F$ computed over each label in that growing set.  For example, the following
%% program implements a {\em level-synchronized} BFS in this way:

\begin{figure}
  \lstinputlisting{chapter2/code/bfs_lvar.hs}
  \caption{\footnotesize 
    An example Haskell program, written using our LVars library, that
    maps a computation over a connected component using a monotonically
    growing set variable.  The code is written in a strict style, using the \lstinline|Par|
    monad for parallelism.  The use of the set variable enables modularity and
    safe pipelining.  Consumers can safely asynchronously execute work items
    put into \lstinline|analyzedSet|.}
  \label{f:bfs-lvar}
\end{figure}

%% \lk{I think that one point that is missing from this section and needs
%%   to be made is how LVars compare with IVars for the purpose of
%%   writing programs like this.  Sure, we know that we can't use IVars
%%   for mark bits, but we obviously didn't end up needing to use mark
%%   bits anyway -- so, what have we really done here that we couldn't
%%   have done (as easily) with IVars?}
%% \rn{I put this in below, it's also hopefully now more obvious above.}


%\begin{figure*}[tb]
\begin{figure*}
 \centering 
%  \includegraphics[clip,trim=200px 200px 300px 200px,width=2.3in]{figures/BFS.pdf} 
%  \includegraphics[width=2.5in]{figures/bargraph_with_speedups2.pdf} 
%  \includegraphics[width=2.7in]{figures/bargraph_with_speedups2.pdf} 

% . . side top
\includegraphics{chapter4/figures/bf_traverse_benchmark_data.png} 
\vspace{-8mm}
  \caption{\footnotesize Running time comparison of Strategies-based and LVar-based
    implementations of \lstinline|bf_traverse|, running on 1, 2, 3, and 4 cores on
    an Intel Xeon Core i5 3.1GHz (smaller is better).}
  %% \caption{\footnotesize Running time comparison of Strategies and of LVar-based
  %%   implementations of \lstinline|bf_traverse|, running on 4 cores on
  %%   an Intel Xeon Core i5 3.1GHz.  Smaller is better; numbers at top of bars indicate
  %%   parallel speedup relative to serial version of the same algorithm.}
  \label{f:performance}
\end{figure*}

%\subsection{Implementations}
\paragraph{A layered implementation}

%% \new{There are {\em three} relevant parties in deploying LVars: the 
%%   runtime implementor, the data structure author who provides an
%%   implementation for a specific lattice (\eg, a monotonically growing hashmap),
%%   and the application writer who uses LVars.  Only the application writer receives
%%   a determinism guarantee.  The data structure author uses
%%   an interface provided by the runtime implementor, who provides core functionality:
%%   thread scheduling and tracking of threads blocked on $\GET$ operations.
%% }

Use of our LVars library typically involves two parties: first, the data
structure author who uses the library directly and provides an
implementation of a specific monotonic data structure (\eg, a
monotonically growing hashmap), and, second, the application writer
who uses that data structure.  Only the application writer receives a
determinism guarantee; it is the data structure author's obligation to ensure
that the states of their data structure form a lattice and that it is only accessible via
the equivalent of $\PUT$ and $\GET$.

The data structure author uses an interface provided by our LVars library,
which provides core runtime functionality: thread scheduling and
tracking of threads blocked on $\GET$ operations.
%% We provide three implementations of our LVar library, each of which can be instantiated
%% with any lattice(s).  All three 
Concretely, 
the data structure author
imports our library
and reexports a limited interface specific to their data structure 
(\eg{}, for sets, @putInSet@ and @waitSetSizeThreshold@).  
%% Again, it is important that entire LVar runtime interface 
%% {\em not} be exported to end users of the data-structure library,
%% because if the states of the data structure do not satisfy
%% the properties of a lattice, things can go awry.  
\new{In fact, our library provides three different runtime interfaces for the data structure author
to choose among.  These ``layered'' interfaces provide the data structure author with a shifting
trade-off between the {\em ease} of meeting their proof obligation, and {\em attainable performance}:}

\begin{enumerate}
\item {\bf Pure LVars}: Here, each LVar is a single mutable
  container (an @IORef@) containing a pure value.  This requires only
  that a library writer select a purely functional data structure and
  provide a @join@\footnote{Type class
    \lstinline|Algebra.Lattice.JoinSemiLattice|.} function for it and a threshold
  predicate for each @get@ operation.  These pure functions are
  easiest to validate, for example, using the popular QuickCheck \cite{quickcheck} tool.

\item {\bf IO LVars}: Pure data structures in mutable containers
  cannot always provide the best performance for concurrent data
  structures.
% \cite{my-haskell-implementors-talk-if-there-is-space}.
  Thus we provide a more effectful interface.  With it, data
  structures are represented in terms of arbitrary mutable state; performing a
  @put@ requires an {\em update action} (@IO@) and a @get@ requires an
  effectful polling function that will be run after any @put@ to the
  LVar, to determine if the @get@ can unblock.

\item {\bf Scalable LVars}: Polling each blocked @get@ upon {\em any}
  @put@ is not very precise.  If the data structure author takes on yet more
  responsibility, they can use our third interface to reduce contention by
  managing
   the storage of blocked computations and threshold functions {\em
     themselves}.  
  For example, a concurrent set
  might store a waitlist of blocked continuations on a per-element basis,
  rather than using one waitlist for the entire LVar, as in layers (1) and (2).
\end{enumerate}

\noindent The support provided to the data structure author {\em declines} with each of
these layers, with the final option providing only parallel scheduling,
and little help with defining the specific LVar data structure.  But
this progressive cost/benefit tradeoff can be beneficial for
prototyping and then refining data structure implementations.  

% \begin{lst}
% putLV :: LVar a -> (a -> IO ()) -> Par ()
% getLV :: LVar a -> (IO (Maybe b)) -> Par b

% pure
% getLV :: LVar a -> (a -> Maybe b) -> Par b
% putLV :: JoinSemiLattice a => LVar a -> a -> Par ()

\paragraph{Revisiting our breadth-first traversal example}

In Section~\ref{section:motivation}, we proposed using LVars to
implement a program that performs a breadth-first traversal of a
connected component of a graph, mapping a function over each node in
the component.
Figure~\ref{f:bfs-lvar} gives a version of this program implemented using our LVars library.
It performs a breadth-first traversal of the 
 @profiles@ graph with effectful $\PUT$
operations on a shared set variable.  
\new{This variable, \lstinline|analyzedSet|, has to be modified
multiple times by \lstinline|putInSet| and thus cannot be an IVar.\footnote{Indeed, there are
  subtle problems with encoding a set even as a linked structure of
  IVars.  For example, if it is represented as a tree, who writes the root?}  The callback function
 \lstinline|analyze| is ``baked into'' \lstinline|analyzedSet| and may run
  as soon as new elements are inserted.  
  %% (In the $\lambdaLVar$ calculus,
  %% \lstinline|analyze| could be built into the $\REIFY$ function
  %% and applied to the result of $\GET$.)
}
Our implementation uses the ``Pure LVars'' runtime layer described above: 
@analyzedSet@ is nothing more than a tree-based data structure
(@Data.Set@) stored in a mutable location.  

\paragraph{Preliminary benchmarking results}

We compared the performance of the LVar-based implementation of @bf_traverse@ against the version in Figure~\ref{f:bfs-pure}, which we ran using the 
\\ @Control.Parallel.Strategies@ library
\cite{marlow-par}, version 3.2.0.3. (\new{Despite being a simple algorithm, even breadth-first search by itself
is considered a useful benchmark; in fact, the well-known
``Graph 500'' \cite{graph500} benchmark is exactly breadth-first search.})

We evaluated the Strategies and LVar versions of @bf_traverse@
by running both on a local random directed graph of 40,000 nodes and
320,000 edges (and therefore an average degree of 8), simulating the
@analyze@ function by doing a specified amount of work for each node,
which we varied from 1 to 32 microseconds.
Figure \ref{f:performance} shows the results of our evaluation on 1, 2, 3, and 4 cores.
Although both the Strategies and LVar versions enjoyed a
 speedup as we added parallel resources, the LVar
version scaled particularly well.
A subtler, but interesting point is that, in the Strategies version,
it took an average of 64.64 milliseconds for the first invocation of
@analyze@ to begin running after the program began, whereas in the
LVar version, it took an average of only 0.18 milliseconds,
indicating that the LVar version allows work to be
pipelined.



\rnote{Report time-to-first-f results in prose I suppose....}


%% \lk{Shouldn't stuff like this be pushed to an evaluation section
%%   further along in the paper?  Not sure it belongs right in the
%%   intro.}
%% \new{In fact, we know it is possible to implement these data structures even more
%% efficiently.  The key question addressed in this paper, is not whether
%% such a system can be implemented, but:}
%The question then becomes: 
%% {\em how might we convince ourselves that every program
%% written with such a library is deterministic, especially when mixing different
%% data structures?}
%% \rn{TODO: More explicitly declare here that the paper is not about this
%%   implementation, but about the underlying theory.}
%
%% The existence of, and usefulness of, practical implementations motivates
%% us to pursue this question.

\lk{Following is dumped in from the POPL paper.}

We have constructed a prototype implementation of LVish as a monadic library in
Haskell, which is available at 
\begin{center}
\url{http://hackage.haskell.org/package/lvish}
\end{center}
\ajt{Or would we rather link to github?}%
Our library adopts the basic approach
of the @Par@ monad~\cite{monad-par}, enabling us to employ our own notion of
lightweight, library-level threads with a custom scheduler.  It supports the
programming model laid out in Section~\ref{section:lvish-informal} in full,
including explicit handler pools.  It differs from our formal model in following
Haskell's by-need evaluation strategy, which also means that concurrency in the
library is \emph{explicitly marked}, either through uses of a @fork@ function or
through asynchronous callbacks, which run in their own lightweight thread.

Implementing LVish as a Haskell library makes it possible to provide
compile-time guarantees about determinism and quasi-determinism,
because programs written using our library run in our @Par@ monad and
can therefore only perform LVish-sanctioned side effects.
We take advantage of
this fact by indexing @Par@ computations with a phantom type
that indicates their \emph{determinism level}:
\begin{lstlisting}
  data Determinism = Det | QuasiDet
\end{lstlisting}
%% while the second is the standard phantom type used by the @ST@ monad, which is
%% used to ensure that LVars themselves cannot escape from uses of our @Par@ monad.
The @Par@ type constructor has the following kind:\footnote{We are here using
  the {\tt DataKinds} extension to Haskell to treat {\tt Determinism} as a
  kind.  In the full implementation, we include a second phantom type parameter to ensure that LVars
  cannot be used in multiple runs of the {\tt Par} monad, in a manner analogous to how the {\tt ST} monad prevents an {\tt STRef} from being returned from {\tt runST}.}
\begin{lstlisting}
  Par :: Determinism -> * -> *
\end{lstlisting}
together with the following suite of @run@ functions:
\begin{lstlisting}
  runPar   :: Par Det a -> a
  runParIO :: Par lvl a -> IO a
  runParThenFreeze :: DeepFrz a => Par Det a -> FrzType a
\end{lstlisting}
The public library API ensures that if code uses @freeze@, it is marked as
@QuasiDet@; thus, code that types as @Det@ is guaranteed to be fully
deterministic.  While LVish code with an arbitrary determinism level @lvl@ can be
executed in the @IO@ monad using @runParIO@, only @Det@ code can be executed as if it were pure,
since it is guaranteed to be free of visible side effects of nondeterminism.  In
the common case that @freeze@ is only needed at the end of an
otherwise-deterministic computation, @runParThenFreeze@ runs the computation to
completion, and then freezes the returned LVar, returning its exact value---and
is guaranteed to be deterministic.\footnote{The {\tt DeepFrz} typeclass is used
  to perform freezing of nested LVars, producing values of frozen type (as given
  by the {\tt FrzType} type function).}

%% The type @s@ (short for ``session'') that
%% appears in the @run@ functions also appears as a phantom type of LVars
%% themselves; the universal quantification forces each LVar to be tied to a single
%% session, \ie, a single use of a @run@ function~\cite{?}.  Readers unfamiliar with this
%% trick from the @ST@ monad can safely ignore the type parameter altogether.

\ajt{We don't actually prove that freeze-free code is deterministic, but it
  should follow pretty easily from the proof of quasi-determinism.  Perhaps we
  should claim it in the proof section?}
\lk{Well, to do this ``right'' we would have to not just prove that
  freeze-free code is deterministic, but we'd have to prove that
  freeze-happens-last code is deterministic...}
