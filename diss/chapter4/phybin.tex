\section{Case study: parallelizing PhyBin with LVish}\label{s:lvish-phybin}

For our second case study, we used the LVish library to parallelize
\emph{PhyBin}, a bioinformatics application for comparing phylogenetic
trees.  A \emph{phylogenetic tree} represents a possible genealogical
ancestry for a set of $N$ species.  Leaf nodes in the tree are labeled
with species' names, and the structure of the tree represents a
hypothesis about common ancestors. For a variety of reasons,
biologists often end up with many alternative trees, whose
relationships they need to then analyze.

PhyBin~\cite{PhyBin} is a medium-sized (3500-line) bioinformatics
program implemented in Haskell\footnote{Available at
  \url{http://hackage.haskell.org/package/phybin}.} for this purpose,
initially released in 2010.  The primary output of the software is a
hierarchical clustering of the input tree set (that is, a tree of
trees), but most of its computational effort is spent computing an $N
\times N$ distance matrix that records the \emph{edit distance}
between each pair of input trees.  It is this distance computation
that we parallelize in our case study.

\subsection{Computing all-to-all tree edit distance}

The distance metric itself is called \emph{Robinson-Foulds} (RF)
distance, and the fastest algorithm for all-to-all RF distance
computation is Sul and Williams' \emph{HashRF}
algorithm~\cite{hashrf}, introduced by a software package of the same
name.\footnote{Available at \url{https://code.google.com/p/hashrf/}.}
The HashRF software package is written in C++ and is about 2-3$\times$
as fast as PhyBin.  Both packages are dozens or hundreds of times
faster than the more widely-used software that computes RF distance
matrices, such as Phylip\footnote{Available at
  \url{http://evolution.genetics.washington.edu/phylip.html}.}~\cite{phylip}
and DendroPy\footnote{Available at
  \url{http://pythonhosted.org/DendroPy/}.}~\cite{dendropy}.  These
slower packages use $\frac{N^2-N}{2}$ full applications of the
distance metric, which has poor locality in that it reads all trees in
from memory $\frac{N^2-N}{2}$ times.

To see how the HashRF algorithm improves on this, consider that each
intermediate node of a tree can be seen as partitioning the the tree's
leaves into two disjoint sets: those below the node, and those above
it.  For example, if a tree has leaves $\setof{a, b, c, d, e}$, one
bipartition would be $\setof{\setof{a, b}, \setof{c, d, e}}$, while
another would be $\setof{\setof{a, b, c}, \setof{d, e}}$.  A tree can
therefore be encoded as a set of bipartitions of its nodes.
Furthermore, once trees are encoded as sets of bipartitions, we can
compute the edit distance between trees (that is, the number of
operations required to transform one tree into the other) by computing
the \emph{symmetric set difference} between sets of bipartitions, and
we can do so using standard set data structures.

The HashRF algorithm makes use of this fact and adds a clever trick
that greatly improves locality.  Before computing the actual distances
between trees, it populates a table mapping each observed bipartition
to the set of trees that contain it.  In the original PhyBin source
code, the type of this table is:

\begin{lstlisting}
  type BipTable = Map DenseLabelSet (Set TreeID)
\end{lstlisting}

Above, a @DenseLabelSet@ encodes an individual bipartition as a bit
vector.  PhyBin uses purely functional data structures for the @Map@
and @Set@ types, whereas HashRF uses a mutable hash table.  Yet in
both cases, these structures grow monotonically during execution,
making PhyBin a good candidate for parallelization.

The full algorithm for computing the distance matrix is shown in
Figure~\ref{f:hashrf-alg}.  The second phase of the algorithm is still
$O(N^2)$, but it only needs to read from the much smaller @trset@
during this phase.  All loops in Figure~\ref{f:hashrf-alg} are
potentially parallel.

\begin{figure}
\begin{lstlisting}
  global: biptable, distmat
  (1) for t `member` alltrees:
        for bip `member` t:
          insert(biptable, (t, bip))
  (2) for (_, trset) `member` biptable: 
        for t1 `member` alltrees:
          for t2 `member` alltrees:
            if t1 `member` trset `xor` t2 `member` trset
            then increment(distmat[t1,t2])
\end{lstlisting}  
  \caption{Pseudocode of the HashRF algorithm for computing a tree
    edit distance matrix.}
  \label{f:hashrf-alg}
\end{figure}

\subsection{Parallelizing the HashRF algorithm with LVish}

\TODO{More text about what the monotonic data structures here are.
  Look at Ryan's talk?}

The LVish approach applies directly to this application: 
\begin{itemize}
\item The @biptable@ in the first phase is a map of sets, which are
  directly replaced by their LVar counterparts.
\item The @distmat@ in the second phase is a vector of monotonic
      @bump@ counters.
\end{itemize}

In fact, the parallel port of PhyBin using LVish was so
straightforward that, after reading the code, parallelizing the first
phase took only 29 minutes.\footnote{Git commit range:
  \url{https://github.com/rrnewton/PhyBin/compare/5cbf7d26c07a...6a05cfab490a7a}.}
Once the second phase was ported, the distance computation sped up by
a factor of $3.35\times$ on 8 cores (Table~\ref{t:phybin-bench}).

This is exactly where we would like to use LVish---to achieve modest
speedups for modest effort, in programs with complicated data
structures (and high allocation rates), and without changing the
determinism guarantee of the original functional code.

\begin{table}
\begin{tabular}{| l | l | l | l | l | l | l |}
\hline
Trees     & Species    & \multicolumn{3}{|l|}{PhyBin} & DendroPy & Phylip \\ \hline
100       & 150        & \multicolumn{3}{|l|}{0.269}  & 22.1     & 12.8   \\ \hline
          &            & \multicolumn{4}{|l|}{PhyBin 1, 2, 4, 8 core} & HashRF \\ \hline
1000      & 150        & 4.7      & 3     & 1.9     & 1.4      & 1.7      \\ \hline
\end{tabular}
\caption{PhyBin performance comparison with DendroPy, Phylip, and HashRF.  All
  times in seconds.}
\label{t:phybin-bench}
\end{table}
