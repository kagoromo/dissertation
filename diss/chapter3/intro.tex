The LVars programming model presented in Chapter~\ref{ch:lvars} is
based on the idea of \emph{monotonic data structures}, in which (1)
information can only be added, never removed, and (2) the order in
which information is added is not observable.  A paradigmatic example
is a set that supports insertion but not removal, but there are many
others.  In the LVars model, all shared data structures (called LVars)
are monotonic, and the states that an LVar can take on form a
\emph{lattice}.  Writes to an LVar must correspond to a least upper
bound operation in the lattice, which means that they monotonically
increase the information in the LVar, and that they commute with one
another.  But commuting writes are not enough to guarantee
determinism: if a read can observe whether or not a concurrent write
has happened, then it can observe differences in scheduling.  So, in
the LVars model, the answer to the question ``has a write occurred?''
(\ie, is the LVar above a certain lattice value?)  is always
\emph{yes}; the reading thread will block until the LVar's contents
reach a desired threshold.  In a monotonic data structure, the absence
of information is transient---another thread could add that
information at any time---but the presence of information is forever.

The LVars model guarantees determinism and supports an unlimited
variety of shared data structures: anything viewable as a lattice.
However, it is not as general-purpose as one might hope.  Consider
again the challenge problem for deterministic-by-construction parallel
programming that we saw in Chapter~\ref{ch:lvars}:
\begin{quote}
  In a directed graph, find the connected component containing a
  vertex $v$, and compute a (possibly expensive) function $f$ over all
  vertices in that component, making the set of results available
  asynchronously to other computations.
\end{quote}
In Section~\ref{s:lvars-motivation}, we considered an attempt at a
solution for this problem using purely functional parallelism (shown
in Figure~\ref{f:bfs-pure}).  Unfortunately, our purely functional
attempt did not quite satisfy the above specification: although
connected component discovery proceeded in parallel, members of the
output set did not become available to other computations until the
entire connected component had been traversed, limiting parallelism.

What would happen if we tried to solve this problem using LVars?  A
typical implementation of the graph-traversal part of the
problem---including the purely functional implementation of
Section~\ref{s:lvars-motivation}---involves a monotonically growing
set of ``seen nodes''; neighbors of seen nodes are fed back into the
set until it reaches a fixed point.  Such fixpoint computations are
ubiquitous, and would seem to be a perfect match for the LVars model
due to their use of monotonicity.  But they are not expressible using
the threshold read and least-upper-bound write operations of the basic
LVars model.

The problem is that these computations rely on \emph{negative}
information about a monotonic data structure, \ie, on the
\emph{absence} of certain writes to the data structure.  In a graph
traversal, for example, neighboring nodes should only be explored if
the current node is \emph{not yet} in the set; a fixpoint is reached
only if no new neighbors are found; and, of course, at the end of the
computation it must be possible to learn exactly which nodes were
reachable (which entails learning that certain nodes were not).  But
in the LVars model, asking whether a node is in a set means waiting
until the node \emph{is} in the set, and it is not clear how to lift
this restriction while retaining determinism.

In this chapter, I describe two extensions to the basic LVars model of
Chapter~\ref{ch:lvars} that make it possible for monotonic data
structures to ``say no'':

\begin{itemize}
\item First, I extend the model with a primitive operation @freeze@
  for \emph{freezing} an LVar, which comes with the following
  tradeoff: once an LVar is frozen, any further writes that would
  change its value instead throw an exception; on the other hand, it
  becomes possible to discover the exact value of the LVar, learning
  both positive and negative information about it, without blocking.
\item
  Second, I add the ability to attach \emph{event handlers} to an
  LVar.  When an event handler has been registered with an LVar, it
  invokes a \emph{callback function} to run asynchronously, whenever
  events arrive (in the form of monotonic updates to the LVar).
  Ordinary LVar reads encourage a synchronous, \emph{pull} model of
  programming in which threads ask specific questions of an LVar,
  potentially blocking until the answer is ``yes''.  Handlers, by
  contrast, support an asynchronous, \emph{push} model of programming.
  Crucially, it is possible to check for \emph{quiescence} of a
  handler, discovering that no callbacks are currently enabled---a
  transient, negative property.  Since quiescence means that there are
  no further changes to respond to, it can be used to tell that a
  fixpoint has been reached.
\end{itemize}

Unfortunately, freezing does not commute with writes that change an
LVar.\footnote{The same is true for quiescence detection; see
Section~\ref{subsection:quasi-quiescence}.}  If a freeze is
interleaved before such a write, the write will raise an exception; if
it is interleaved afterwards, the program will proceed normally.  It
would appear that the price of negative information is the loss of
determinism!

Fortunately, the loss is not total.  Although LVar programs with
freezing are not guaranteed to be deterministic, they do satisfy a
related property that I call \emph{quasi-determinism}: all executions
that produce a final value produce the \emph{same} final value.  To
put it another way, a quasi-deterministic program can be trusted to
never change its answer due to nondeterminism; at worst, it might
raise an exception on some runs.  This exception can in principle
pinpoint the exact pair of freeze and write operations that are
racing, greatly easing debugging.

In general, the ability to make exact observations of the contents of
data structures is in tension with the goal of guaranteed determinism.
Since pushing towards full-featured, general monotonic data structures
leads to flirtation with nondeterminism, perhaps the best way of
ultimately getting deterministic outcomes is to traipse a short
distance into nondeterministic territory, and make our way back.  The
identification of quasi-deterministic programs as a useful
intermediate class of programs is a contribution of this dissertation.
That said, in many cases the @freeze@ construct is only used as the
very final step of a computation: after a global barrier, freezing is
used to extract an answer.  In this common case, determinism is
guaranteed, since no writes can subsequently occur.

I will refer to the LVars model, extended with handlers, quiescence,
and freezing, as the \emph{LVish model}.  The rest of this chapter is
organized as follows: in Section~\ref{s:quasi-informal}, I introduce
the LVish programming model informally, through a series of examples.
I will also return to the graph traversal I discussed above and show
how to implement it using the LVish Haskell library (which I will go
on to discuss the internals of in Chapter~\ref{ch:lvish}).

Then, in Section~\ref{s:quasi-formal}, I formalize the LVish model by
extending the $\lambdaLVar$ calculus of Chapter~\ref{ch:lvars} to add
support for handlers, quiescence, and freezing, calling the resulting
language $\lambdaLVish$.  The main technical result of this chapter is
a proof of quasi-determinism for
$\lambdaLVish$~(Section~\ref{s:quasi-proof}). Just as with the
determinism proof I gave for $\lambdaLVar$ in Chapter~\ref{ch:lvars},
the quasi-determinism proof for $\lambdaLVish$ relies on an
Independence lemma (Section~\ref{subsection:quasi-independence}) that
captures the commutative effects of LVar computations.
